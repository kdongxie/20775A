### 1. 제공된 VideoShop.sql파일을 이용하여 MS Sql Server에 VideoShop을 생성하고 Sqoop을 사용하여 VS_TAPE을 HDFS에 Import/Export하는 식을 작성하시오.
* 접속확인

```
* Microsoft Sql Server
./bin/sqoop list-databases --connect jdbc:sqlserver://70.12.114.147:1433 --username 'sa' --password '1234'
```

* VS_CUSTOMER Table을 Import하는 Query [Map Reduce작업을 수행한다]

```
./bin/sqoop import --connect 'jdbc:sqlserver://70.12.114.147:1433;database=VideoShop' --username 'sa' --password 'Pa$$w0rd1234' --table VS_CUSTOMER -m 1 --target-dir /user/hadoop/sqoopMsSqlToHdfs
```

* VS_TAPE Table을 Import하는 Query

```
./bin/sqoop import --connect 'jdbc:sqlserver://70.12.114.147:1433;database=VideoShop' --username 'sa' --password 'Pa$$w0rd1234' --table VS_TAPE -m 1 --target-dir /user/hadoop/sqoopMsSqlToHdfs2
```

```
## 작업을 나타내어 준다
19/04/01 10:18:09 INFO mapreduce.Job:  map 0% reduce 0%
19/04/01 10:18:17 INFO mapreduce.Job:  map 100% reduce 0%
19/04/01 10:18:18 INFO mapreduce.Job: Job job_1554077910911_0002 completed successfully
19/04/01 10:18:18 INFO mapreduce.Job: Counters: 30
        File System Counters
                FILE: Number of bytes read=0
                FILE: Number of bytes written=208112
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=87
                HDFS: Number of bytes written=1589
                HDFS: Number of read operations=4
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=2
        Job Counters
                Launched map tasks=1
                Other local map tasks=1
                Total time spent by all maps in occupied slots (ms)=5273
                Total time spent by all reduces in occupied slots (ms)=0
                Total time spent by all map tasks (ms)=5273
                Total vcore-milliseconds taken by all map tasks=5273
                Total megabyte-milliseconds taken by all map tasks=5399552
        Map-Reduce Framework
                Map input records=18
                Map output records=18
                Input split bytes=87
                Spilled Records=0
                Failed Shuffles=0
                Merged Map outputs=0
                GC time elapsed (ms)=84
                CPU time spent (ms)=1260
                Physical memory (bytes) snapshot=144805888
                Virtual memory (bytes) snapshot=2085724160
                Total committed heap usage (bytes)=18751488
        File Input Format Counters
                Bytes Read=0
        File Output Format Counters
                Bytes Written=1589
19/04/01 10:18:18 INFO mapreduce.ImportJobBase: Transferred 1.5518 KB in 27.1916 seconds (58.4372 bytes/sec)
19/04/01 10:18:18 INFO mapreduce.ImportJobBase: Retrieved 18 records.
```



* VS_CUSTOMER Table을 Export하는 Query

```
./bin/sqoop export --connect 'jdbc:sqlserver://70.12.114.147:1433;database=VideoShop' --username 'sa' --password 'Pa$$w0rd1234' --table VS_TAPE -m 1 --export-dir /user/hadoop/sqoopMsSqlToHdfs/part-m-00000
```





### 3. 전국 커피숍 년도별 폐업건수를 출력하시오.  (coffee.csv파일)

```R
df <- read.csv("coffee.csv", encoding = "utf-8", stringsAsFactors = FALSE)
sub_year<-subset(df,df$stateOfbusiness == "폐업 등", select= c(yearOfStart,stateOfbusiness))
sub_year
coffee_data <- sub_year[order(sub_year$yearOfStart),]
coffee_data
```

### 4. 연령대별로 암의 발생률을 구하고 그래프로 출력하시오.  (cancer.csv파일)

```R
install.packages("ggplot2")
library(ggplot2)
df2 <- read.csv("cancer.csv", encoding = "utf-8", stringsAsFactors = FALSE)
View(df2)
table_age <- cut(df2$age,breaks = (1:11)*10)
plot(table_age)
```

![image](https://user-images.githubusercontent.com/46669551/55300529-df3f8e00-5472-11e9-9ea5-db10843a6522.png)

### 5. Hadoop MapReduce를 처리하시오. (ab40thv.txt파일)

1. Apache Spark shell을 실행시킨다

```
[hadoop@server10 spark-2.3.3-bin-hadoop2.7]$ ./bin/spark-shell
```

2. 파일을 호출하여 변수에 저장한다. 

```scala
val input = sc.textFile("ab40thv.txt")
```

```
input: org.apache.spark.rdd.RDD[String] = /usr/local/lib/spark/ab40thv.txt MapPartitionsRDD[1] at textFile at <console>:24
```

3. 불러들인 파일 Map Reduce 하기

```scala
val input_map = input.flatMap(x=>x.split(" ")).map(word=>(word,1)).reduceByKey((a, b) => a + b)
```

```scala
val wordcounts = input_map.reduceByKey((a, b) => a + b)
```

```scala
wordcounts.collect()
```

6. Apache Spark를 사용하여 제공하는 baby_names의 데이터를 이용하여 년도별,성별 아기 출생건수를 구하시오.
     (baby_names.csv파일)

```scala
import org.apache.spark.sql.SQLContext
import org.apache.spark.{SparkConf, SparkContext}
```

```
val baby = sc.textFile("baby_names.csv")

  
```

### HDFS를 통한 Data MapReduce실행 [PySpark ]

```python
  http://server10:8080/
 
 파일을 hadoop경로에 폴더를 생성하여 넣어준다.
    1. 폴더생성
 $ bin/hadoop fs -mkdir /user/hadoop/spark_data
	2. text file을 이동
 $ bin/hadoop fs -put ~/ab40thv.txt /user/hadoop/spark_data/
	3. csv file을 이동
 $ bin/hadoop fs -put ~/baby_names.csv /user/hadoop/spark_data/
	4. 경로 확인
 $ bin/hadoop fs -ls /user/hadoop/spark_data
 
 Spark에서 sbin/start-all.sh 를 실행하여 jps의 Master를 구동시켜준다.
 
 $ ./bin/pyspark --master spark://server10:7077 의 명령을 통해 Spark를 실행 시켜준다.
 # Map Reduce를 진행
 >>> data = sc.textFile("파일 루트")
 >>> data = sc.textFile("hdfs://server10:9000/user/hadoop/spark_data/ab40thv.txt")
 >>> data.collect()
 >>> data2 = data.flatMap(lambda line : line.split(" "))
 >>> data3 = data2.map(lambda word: (word,1))
 >>> data4 = data3.reduceByKey(lambda a, b: a+b)
 >>> data4.collect()
    
 # 한줄로도 가능 
data = sc.textFile("hdfs://server10:9000/user/hadoop/spark_data/ab40thv.txt")
wordcount = data.flatMap(lambda line : line.split(" ")).map(lambda word: (word,1)).reduceByKey(lambda a, b: a+b)
```



```python
# CSV File을 불러들이기
>>>baby_names = sqlContext.read.format("com.databricks.spark.csv").options(header='true', inferschema='true').load('hdfs://server10:9000/user/hadoop/spark_data/baby_names.csv')

# Data 가공하기
 >>> baby_names.registerTempTable("baby_names")
 >>> result = sqlContext.sql("select * from baby_names")
 >>> result.show()
 >>> result2 = sqlContext.sql("select Year, Sex, count(*) FROM baby_names group by Year, Sex")
 >>> result3 = sqlContext.sql("select Year, count(*) FROM baby_names group by Year, Sex")
 >>> result2.show()
 >>> result3.show()
```

### R을 이용한 baby_names Data File 조작하기

```R
install.packages("dplyr")
library("dplyr")
df5 <- read.csv("baby_names.csv", fileEncoding = 'utf-8',stringsAsFactors = FALSE)
df6 <- data.frame(df5)
head(df6)
summarise(group_by(df6,Year,Sex),Birth_sum=var(Count))
df7 <- summarise(group_by(df6,Year,Sex),Birth_sum=var(Count))
barplot(df7$Birth_sum,horiz=T,col=c('red','green'),names=df7$Year, main = '연도별 출생아 수')
```

```R
# A tibble: 18 x 3
# Groups:   Year [?]
    Year Sex   Birth_sum
   <int> <chr>     <dbl>
 1  2007 F         345  
 2  2007 M         759  
 3  2008 F         319  
 4  2008 M         742  
 5  2009 F         345  
 6  2009 M         732  
 7  2010 F         343  
 8  2010 M         720  
 9  2011 F         344  
10  2011 M         684  
11  2012 F         346  
12  2012 M         652  
13  2013 F         360  
14  2013 M         607  
15  2014 F         265  
16  2014 M         504  
17  2015 F          32.0
18  2015 M          71.2
```

![image](https://user-images.githubusercontent.com/46669551/55309211-11afb200-5498-11e9-9558-a264cdc0734a.png)